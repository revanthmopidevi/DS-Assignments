{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to Ponder\n",
    "__1. What happens to the training and testing error as polynomials of higher degree are used for prediction?__\n",
    "<br>\n",
    "As the polynomials of higher degree are used, then the training error will decrease for a small sample size, this is because the model is able to fit according to a small number of samples  in the training data and predict them perfectly. But, the same model has really high testing error, this is because it is trained to very specific data points and it cannot make generalised predictions for new data.\n",
    "<br><br>\n",
    "__2.Does a single global minimum exist for Polynomial Regression as well? If yes, justify.__\n",
    "<br>\n",
    "Yes, a single global minimum exists for polynomial regression as well. Any polynomial regression equation can be solved to obtain final weights in order to minimise the error by differentiating w.r.t. the weights w0,w1,....wNN.\n",
    "<br><br>\n",
    "__3. Which form of regularization curbs overfitting better in your case? Can you think of a case when Lasso regularization works better than Ridge?__\n",
    "<br>\n",
    "Ridge regularisation curbs slightly better overfitting than lasso regression.\n",
    "<br>\n",
    "Lasso regularisation works better than ridge regularisation when there are a small number of significant parameters and the others are close to zero I.e. only few predictors will properly influence the response.\n",
    "<br>\n",
    "Ridge regularisation works well if there are many large parameters of about the same value I.e. when most of the predictors impact the response.\n",
    "<br><br>\n",
    "__4. How does the regularization parameter affect the regularization process and weights? What would happen if a higher value for Î» (> 2) was chosen?__\n",
    "<br>\n",
    "When the regularisation parameter is larger, more penalty will be assigned to larger weights for features, this implies that extent of overfitting is inversely proportional to regularisation parameter. This regularisation parameter helps to tackle overfitting.\n",
    "<br>\n",
    "If a higher lambda value is chosen, high penalties will be assigned to weights of features. As we increase the lambda value, the model starts to underfit the data and slowly the weights of some features will become insignificant I.e. they tend to become zero. For a much higher lambda ( > 100), the regression line is almost parallel to the x-axis since only theta zero significantly contributes to the equation. In such a case, testing and training errors are extremely high.\n",
    "<br><br>\n",
    "__5. Regularization is necessary when you have a large number of features but limited training instances. Do you agree with this statement?__\n",
    "<br>\n",
    "Yes, we agree with this statement. When training samples are of low numbers, the regression will try to fit the data points perfectly and the coefficients of the features become large. Variance will be high, when the same weights are used to make predictions on testing data. Hence, regularisation term is used to penalise the large parameters which leads to overfitting.\n",
    "<br><br>\n",
    "__6. If you are provided with D original features and are asked to generate new matured features of degree N, how many such new matured features will you be able to generate? Answer in terms of N and D.__\n",
    "<br>\n",
    "Number of features => $(D,N) = (D + N)_{C_{N}}$\n",
    "<br>\n",
    "where D is the number of original features. N is degree of polynomial.\n",
    "<br><br>\n",
    "__7. What is bias-variance trade off and how does it relate to overfitting and regularization.__\n",
    "<br>\n",
    "A model with high bias (I.e. training error) and high variance (I.e. testing error) will face the problem of under-fitting.\n",
    "<br>\n",
    "Similarly, the model with low bias and high variance will face the problems of Overfitting.\n",
    "<br>\n",
    "A model with low bias and low variance is always a good model. This is called Bias-Variance trade-off. In Order to arrive at a optimal Bias-Variance trade-off, the degree of polynomial and the regularisation parameter need to be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
