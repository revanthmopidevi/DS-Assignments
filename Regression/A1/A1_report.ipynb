{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing:\n",
    "Given 1338 tuples of data containing three feature attributes and one target attribute. Before the model was trained, the dataset was pre-processed as follows.\n",
    "#### 1. Shuffling the data: \n",
    "The dataset was randomly shuffled to ensure the split into train and test data remains random and the model is general.<br>\n",
    "#### 2. Splitting the data:\n",
    "The dataset was split into training and testing data in 70:30 ratio in order.<br>\n",
    "#### 3. Standardising the data:\n",
    "The feature attributes of the training and testing datasets were standardised separately to prevent leakage of testing data into training data. Finally, the Panda Series were converted into Numpy Arrays for easier and faster computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:\n",
    "The models were generated by solving Normal Equations, by Gradient Descent and by Stochastic Gradient Descent algorithms after each random shuffle and split of the dataset. The intercept of the models was close to 13000 as expected, which is the mean of the target attribute. Error Sum of Squares (SSE) was used as the key performance indicator of the of the predictive models. All the three optimization algorithms yielded similar linear regression models.\n",
    "<br><br>\n",
    "Hypothesis - $y = w_{0} + w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3}$\n",
    "<br><br>\n",
    "$x_{1}, x_{2}, x_{3}$ represent the age, bmi and number of children respectively. $ w_{1},w_{2}, w_{3}$ are weights associated with $x_{1}, x_{2}, x_{3}$.<br><br>\n",
    "$$X = \n",
    "\\begin{bmatrix} \n",
    "1 & x_{11} & x_{12} & x_{13}\\\\\n",
    "1 & x_{21} & x_{22} & x_{23}\\\\\n",
    ". & . & . & . \\\\\n",
    ". & . & . & . \\\\\n",
    "1 & x_{m1} & x_{m2} & x_{m3}\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "$$\n",
    "$$ $$\n",
    "$$Y = \n",
    "\\begin{bmatrix} \n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "y_{m}\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "$$ where $m$ = size of training data\n",
    "$$ $$\n",
    "$$ω = \n",
    "\\begin{bmatrix} \n",
    "ω_{0}\\\\\n",
    "ω_{1}\\\\\n",
    "ω_{2}\\\\\n",
    "ω_{3}\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "$$\n",
    "$$ $$\n",
    "$$b = X^{T} . Y$$\n",
    "Matrix dot product and inversion were performed with Numpy library by first converting Pandas Series to Numpy Array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms:\n",
    "#### Linear Regression by Solving Normal Equations\n",
    "The parial derivatives of error function with respect to each weight was equated to zero. The best fit line was derived by solving for these equations for the weights by means of linear algebra.The model with the least error by solving normal eqations had the following weights:\n",
    "<br>\n",
    "__$ω_{0}$ = 13072.157252916671, $ω_{1}$ = 3173.6711350795026, $ω_{2}$ = 2172.652465579652 and $ω_{3}$ = 569.7149647829754__\n",
    "<br><br>\n",
    "$$ ω = (X^{T}.X)^{-1}x^{T}.Y = (X^{T}.X)^{-1}.b$$\n",
    "#### Linear Regression by Gradient Descent\n",
    "Gradient Descent algorithm was implemented with SSE as the error function on the hypothesis. __np.random.randn()__ function was used to initialize the weights to random values. The learning rate used for the same was $10^{-7}$, and the model converged to a error sum of squares of 6.086349e+10 after approximately 140,000 iterations, the weights obtained corresponding to the least training error were:\n",
    "__$ω_{0}$ = 13072.142207938474, $ω_{1}$ = 3173.667317195016,  $ω_{2}$ = 2172.653751292538 and $ω_{3}$ = 569.7173903721999__\n",
    "$$\n",
    "Sum of Squares of Errors =\n",
    "E(ω) = \n",
    "\\begin{equation}\n",
    "\\frac{1}{2} * \\sum_{n=0}^{N} (x_{n}*ω - y_{n})^{2} \n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E(ω)}{\\partial ω} = (X.ω - Y).X\n",
    "\\end{equation}\n",
    "$$\n",
    "$$ $$\n",
    "$$ Gradient$$\n",
    "$$ $$\n",
    "$$\n",
    "ω = ω - η * \\frac{\\partial E(ω)}{\\partial ω}\n",
    "$$\n",
    "$$ $$\n",
    "where $η$ is the learning rate\n",
    "#### Linear Regression by Stochastic Gradient Descent\n",
    "SGD made sequential passes over the training data, and during each pass, updated feature weights one example at a time with the aim of approaching the optimal weights that minimize the loss. __np.random.randn()__ function was used to initialize the weights to random values. The model converged to a minimum training error of 5.868913e+10\n",
    "after around 850,000 iterations with the corresponding weights as follows:\n",
    "<br>\n",
    "__$ω_{0}$ = 13071.179444392286, $ω_{1}$ = 3174.536142389335, $ω_{2}$ = 2174.1187630838886, $ω_{3}$ = 570.4156365220362__\n",
    "$$$$\n",
    "$$ \n",
    "\\begin{equation}\n",
    "\\frac{\\partial E(ω)}{\\partial ω}_{ω=ω_{n}} = (x_{n}.ω - y_{n}).x_{n}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$ $$\n",
    "$$ Gradient$$\n",
    "$$ $$\n",
    "$$\n",
    "ω = ω - η * \\frac{\\partial E(ω)}{\\partial ω}_{ω=ω_{n}}\n",
    "$$\n",
    "$$ $$\n",
    "where $η$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean, Variance and Minimum of Training Error\n",
    "#### Normal Equations\n",
    "Mean of training errors obtained over 20 regression models = 60863489090.22312<br>\n",
    "Variance of training errors obtained over 20 regression models = 2.3592510523138785e+18<br>\n",
    "Minimum training errors obtained over 20 regression models = 58689130587.04303<br>\n",
    "#### Gradient Descent\n",
    "Mean of training errors obtained over 20 regression models = 60863489090.34578<br>\n",
    "Variance of training errors obtained over 20 regression models = 2.359251052323336e+18<br>\n",
    "Minimum of training errors obtained over 20 regression models = 58689130587.1583<br>\n",
    "#### Stochastic Gradient Descent\n",
    "Mean of training errors obtained over 20 regression models = 60863507490.357376<br>\n",
    "Variance of training errors obtained over 20 regression models = 2.3592502939019807e+18<br>\n",
    "Minimum training errors obtained over 20 regression models = 58689132776.446106<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean, Variance and Mean of Testing Error\n",
    "#### Normal Equations\n",
    "Mean of testing error obtained over 20 regression models = 25548673963.726776<br>\n",
    "Variance of training error obtained over 20 regression models = 2.2762131308133821e+18<br>\n",
    "Minimum testing error obtained over 20 regression models = 22987187804.050194<br>\n",
    "#### Gradient Descent\n",
    "Mean of testing error obtained over 20 regression models = 25548672100.185345<br>\n",
    "Variance of testing error obtained over 20 regression models = 2.2762222241018094e+18<br>\n",
    "Minimum testing error obtained over 20 regression models = 22987179664.889412<br>\n",
    "#### Stochastic Gradient Descent\n",
    "Mean of testing error obtained over 20 regression models = 25548244755.53107<br>\n",
    "Variance of training error obtained over 20 regression models = 2.279184996206161e+18<br>\n",
    "Minimum testing error obtained over 20 regression models = 22982193889.584408<br>\n",
    "<br>\n",
    "The large error values are obtained due to the choice of the model's key performance indicator, namely, sum of squares fo errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Convergence: Error vs Epochs\n",
    "__Plot of $E(\\omega)$ against the number of iterations of Gradient Descent__<br>\n",
    "<img src=\"GD_error_epoch.jpeg\" alt=\"GD_error_epoch.jpeg\"><br>\n",
    "__Plot of $E(\\omega)$ against the number of iterations of Stochastic Gradient Descent__\n",
    "<img src=\"SGD_error_epoch.jpeg\" alt=\"SGD_error_epoch.jpeg\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Questions\n",
    "All three methods yield very close models. Normal Equations and Gradient Descent methods differ only in decimals while Stochastic Gradient Descent differs by a small value. The result is because the error function has a single global minima, and all three algorithms aim to converge to it. Since the minima is unique, the models converge to the same weights. However, the small discrepancy is because the gradient and stochastic gradient descent algorithms are stopped prematurely when the difference in gradient calculated is too small to result in a significant change in the model. The most efficient algorithm to work with would be Stochastic Gradient Descent, which reaches the minima faster than other algorithms when the dataset is huge.<br><br>\n",
    "Standardization is a scaling technique where the values are centred around the mean, i.e. mean of the distribution becomes zero with a unit standard deviation. In multivariate regression, standardization brings variables having different units to comparable units, better ensuring that all the weights are updated at similar rates and a more accurate predictive model.\n",
    "<br><br>\n",
    "Stochastic gradient descent plots converged to the minima significantly faster than gradient descent algorithm. Approximately, it took SGD 8000 epochs, while it took GD 140,000 epochs to achieve the same. However, the GD algorithm yielded a more accurate model. In real-world data, SGD is a more practical way of implementing linear regression.\n",
    "In general, increasing the number of training iterations tends to minimize the loss function and resulting in a more accurate model. However, after a large number of epochs, the change in error function becomes negligible, and the computational complexity of running more iterations would outweigh the resulting predictive accuracy of the model\n",
    "<br><br>\n",
    "If a large learning rate is used in the GD/SGD algorithm, the error value will overshoot the minima of the loss function. It may keep diverging from the minima indefinitely, thereby resulting in an infinite loop.\n",
    "<br><br>\n",
    "If the model does not have a bias term, it'll be equal to zero when all the variables are zero. However, the mean of the 'charges' variable is equal to 13000. Therefore such a model would not fit the data well and result in larger error values. The minima would have been larger without a bias term.\n",
    "<br><br>\n",
    "The final vector value signifies the linear model that best fits the training data, given the initial hypothesis. Since the feature attributes are standardized, they update at similar rates. Noticeably, $ω_{1}$ has the largest value among all the weights, excluding bias value, meaning the model is susceptible to bigger change due to $ω_{1}$ given the same difference in all feature attributes. Therefore, 'age' has the greatest influence on the target attribute. Similarly, it can be deduced that 'children' has the least impact on the target attribute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
