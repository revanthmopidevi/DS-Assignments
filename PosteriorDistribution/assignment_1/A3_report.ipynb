{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 160 entries - each outcome representing a coin toss. x = 1 represents a head and x = 0, a tail. The dataset has been randomly generating using numpy libraries. The maximum likelihood estimator has been generated using numpy.random.rand() function and the corresponding dataset has been generated using numpy.random.choice() function. To ensure maximum likelihood estimator does not fall in the range [0.4, 0.6], uml is randomly generated while it falls in that range. A random dataset is generated every time the notebook is run.<br><br>\n",
    "The fraction of data having x = 1, represented by Bernoulli distribution and binomial distribution give extremely over-fitted results for small datasets. In order to incorporate a Bayesian viewpoint, the domain experts knowledge of the inherent bias of the coin is introduced as the prior distribution over parameter µ. Beta distribution is chosen as the prior as it exhibits conjugacy property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Distribution\n",
    "$$p(𝜇| 𝑎, 𝑏) = 𝐵𝑒𝑡𝑎(𝜇| 𝑎, 𝑏) = \\frac{𝛤(𝑎 + 𝑏)}{{𝛤(𝑎)}{𝛤(𝑏)}}  𝜇 ^{𝑎−1}  (1 − 𝜇) ^{𝑏−1}$$<br>\n",
    "$$E[µ] = \\frac{a}{a + b} $$<br>\n",
    "__Given, $E[µ] = 0.4$. Therefore, $a = \\frac{2}{3} b$. The values of $a$ and $b$ are taken as $2$ and $3$ respectively.__\n",
    "<img src=\"prior_distribution.jpeg\" alt=\"kindly put all the files in same directory to display the Image\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Distribution\n",
    "__Posterior distribution of µ is obtained by multiplying the beta prior by the binomial likelihood function and normalizing.__<br><br>\n",
    "$$p(µ|m, l, a, b) ∝ µ^{m+a−1}(1 − µ)^{l+b−1}$$<br>\n",
    "$$p(µ|m, l, a, b) = \\frac{Γ(m + a + l + b)}{Γ(m + a)Γ(l + b)} µ^{m+a−1}(1 − µ)^{l+b−1}$$<br>\n",
    "$${Posterior} {Mean} {Value} = E[µ] = \\frac{a + m}{a + m + b + l} $$<br>\n",
    "where $m$ = number of heads in the dataset. $l$ = number of tails in the dataset = size of dataset - $m$ = 160 - $m$.<br>\n",
    "<img src=\"posterior_final.jpeg\" alt=\"kindly put all the files in same directory to display the image\">\n",
    "### Evolution of Posterior in Sequential Learning:\n",
    "<img src=\"posterior_distribution.jpeg\" alt=\"kindly put all the files in same directory to display the image\">\n",
    "<img src=\"sequential_training.gif\" alt=\"kindly put all the files in same directory to display the GIF\">\n",
    "Final hyperparameters of posterior distribution are a = 50, b = 115. Posterior Mean Value obtained is 0.30303030303030304."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities and Differences between sequential approach and likelihood approach:\n",
    "1. The sequential approach incorporates a Bayesian viewpoint. It is independent of the choice of the prior and the likelihood function. It records observations one at a time and discards them before the consideration of the next observation. At each stage, the derived posterior distribution acts as a prior distribution for the following data.<br><br>\n",
    "2. Therefore, the sequential method comes in handy for large datasets for its ability to work with partial data as opposed to the whole dataset. On observing the plot, the beta distribution curve becomes increasingly sharp-peaked with an increase in  the number of observations.<br><br>\n",
    "3. When the whole dataset is available at once, the number of ones and zeros in the dataset respectively updates the parameter values simultaneously.<br><br>\n",
    "4. The posterior mean for µ always lies between the prior mean and the maximum likelihood estimate for µ for a finite dataset.<br><br>\n",
    "5. Both the approaches yield the same final posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Questions:\n",
    "As the number of observations increases, the posterior distribution becomes more sharply peaked and the uncertainty represented by the posterior distribution will steadily decrease.<br><br>\n",
    "If $µ_{ML} = 0.5$, the prior distribution assumes a non-biased coin. Since the dataset is randomly generated, the posterior distribution peaks at approximately $µ = 0.5$, i.e. a biasless coin.<br><br>\n",
    "Bob's model would be more helpful and easier while working with large real time data as it makes use of observations one at a time, or in small batches, and then discard them before the next observations, therefore it does not require the whole data set to be stored or loaded into memory. It can also be employed in real-time learning scenarios where a steady stream of data is arriving, and predictions must be made before all of the data is seen.<br><br>\n",
    "Likelihood function takes the form of $µ^{x}(1 − µ)^{1−x}$, therefore, if a distribution that is proportional to the powers of $µ$ and $(1 − µ)$ is taken as the prior, then the posterior distribution would also have the same functional form. Since Gamma, Guassian or Pareto lack this property, known as conjugacy, the posteriors of their priors would differ functionally and hence computationally more complex."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
