{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 160 entries - each outcome representing a coin toss. x = 1 represents a head and x = 0, a tail. The dataset has been randomly generating using numpy libraries. The maximum likelihood estimator has been generated using numpy.random.rand() function and the corresponding dataset has been generated using numpy.random.choice() function. To ensure maximum likelihood estimator does not fall in the range [0.4, 0.6], uml is randomly generated while it falls in that range. A random dataset is generated every time the notebook is run.<br><br>\n",
    "The fraction of data having x = 1, represented by Bernoulli distribution and binomial distribution give extremely over-fitted results for small datasets. In order to incorporate a Bayesian viewpoint, the domain experts knowledge of the inherent bias of the coin is introduced as the prior distribution over parameter Âµ. Beta distribution is chosen as the prior as it exhibits conjugacy property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Distribution\n",
    "$$p(ğœ‡| ğ‘, ğ‘) = ğµğ‘’ğ‘¡ğ‘(ğœ‡| ğ‘, ğ‘) = \\frac{ğ›¤(ğ‘ + ğ‘)}{{ğ›¤(ğ‘)}{ğ›¤(ğ‘)}}  ğœ‡ ^{ğ‘âˆ’1}  (1 âˆ’ ğœ‡) ^{ğ‘âˆ’1}$$<br>\n",
    "$$E[Âµ] = \\frac{a}{a + b} $$<br>\n",
    "__Given, $E[Âµ] = 0.4$. Therefore, $a = \\frac{2}{3} b$. The values of $a$ and $b$ are taken as $2$ and $3$ respectively.__\n",
    "<img src=\"prior_distribution.jpeg\" alt=\"prior_distribution.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Distribution\n",
    "__Posterior distribution of Âµ is obtained by multiplying the beta prior by the binomial likelihood function and normalizing.__<br><br>\n",
    "$$p(Âµ|m, l, a, b) âˆ Âµ^{m+aâˆ’1}(1 âˆ’ Âµ)^{l+bâˆ’1}$$<br>\n",
    "$$p(Âµ|m, l, a, b) = \\frac{Î“(m + a + l + b)}{Î“(m + a)Î“(l + b)} Âµ^{m+aâˆ’1}(1 âˆ’ Âµ)^{l+bâˆ’1}$$<br>\n",
    "$${Posterior} {Mean} {Value} = E[Âµ] = \\frac{a + m}{a + m + b + l} $$<br>\n",
    "where $m$ = number of heads in the dataset. $l$ = number of tails in the dataset = size of dataset - $m$ = 160 - $m$.<br>\n",
    "<img src=\"posterior_final.jpeg\" alt=\"posterior_final.jpeg\">\n",
    "### Evolution of Posterior in Sequential Learning:\n",
    "<img src=\"posterior_distribution.jpeg\" alt=\"posterior_distribution.jpeg\">\n",
    "<img src=\"sequential_training.gif\" alt=\"sequential_training.gif\">\n",
    "Final hyperparameters of posterior distribution are a = 50, b = 115. Posterior Mean Value obtained is 0.30303030303030304."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities and Differences between sequential approach and likelihood approach:\n",
    "### Differences\n",
    "1. The sequential approach incorporates a Bayesian viewpoint. It is independent of the choice of the prior and the likelihood function. It records observations one at a time and discards them before the consideration of the next observation. At each stage, the derived posterior distribution acts as a prior distribution for the following data. Whereas when the whole dataset is available at once, the number of ones and zeros in the dataset respectively update the parameter values at once.\n",
    "2. Only Sequential approach works in the case of real-time learning scenarios where a steady stream of data is arriving, and predictions must be made before all of the data is seen.\n",
    "\n",
    "### Similarities\n",
    "1. Both the approaches yield the same final posterior distribution. The posterior mean for Âµ always lies between the prior mean and the maximum likelihood estimate for Âµ for a finite dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of observations increases, the posterior distribution becomes more sharply peaked and the variance and uncertainty represented by the posterior distribution will steadily decrease. In the limit of an infinitely large data set m, l â†’ âˆ the result reduces to the maximum likelihood result.<br><br>\n",
    "If $Âµ_{ML} = 0.5$, the prior distribution assumes a non-biased coin. Since the dataset is randomly generated, the posterior distribution peaks at approximately $Âµ = 0.5$, i.e. the final distribution approximately becomes a Guassian with mean 0.5.\n",
    "<br><br>\n",
    "Bob's model would be more helpful and easier while working with large real time data as it makes use of observations one at a time, or in small batches, and then discard them before the next observations, therefore it does not require the whole data set to be stored or loaded into memory. It can also be employed in real-time learning scenarios where a steady stream of data is arriving, and predictions must be made before all of the data is seen.\n",
    "<br><br>\n",
    "Likelihood function takes the form of $Âµ^{x}(1 âˆ’ Âµ)^{1âˆ’x}$, therefore, if a distribution that is proportional to the powers of $Âµ$ and $(1 âˆ’ Âµ)$ is taken as the prior, then the posterior distribution would have the same functional dependance as the prior. If a Beta distribution is taken as prior, the posterior would indeed be another Beta function and its normalization coefficient can be obtained easily. However, since Gamma, Guassian or Pareto lack this property, known as conjugacy, the posteriors of obtained would differ functionally from their priors which will complicate computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
